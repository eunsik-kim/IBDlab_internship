{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-23.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.4.0.dev0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.38.1)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0.dev20230513+cu118)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n",
      "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.4.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.35.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0rc1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: pytorch-triton==2.1.0+7d1a95b046 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0+7d1a95b046)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.2.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.0)\n",
      "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.35.0-py2.py3-none-any.whl (248 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: libclang, flatbuffers, wrapt, werkzeug, threadpoolctl, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, smmap, setproctitle, sentry-sdk, scipy, pyasn1, protobuf, opt-einsum, ml-dtypes, markdown, keras, joblib, h5py, grpcio, google-pasta, gast, et-xmlfile, docker-pycreds, cachetools, astunparse, absl-py, scikit-learn, rsa, requests-oauthlib, pyasn1-modules, openpyxl, gitdb, google-auth, GitPython, wandb, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed GitPython-3.1.40 absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 docker-pycreds-0.4.0 et-xmlfile-1.1.0 flatbuffers-23.5.26 gast-0.5.4 gitdb-4.0.11 google-auth-2.23.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 joblib-1.3.2 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 openpyxl-3.1.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.3.2 scipy-1.11.4 sentry-sdk-1.35.0 setproctitle-1.3.3 smmap-5.0.1 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 threadpoolctl-3.2.0 wandb-0.16.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \u001b[0m\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]3m\u001b[33m\u001b[33m\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1011 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]       \u001b[0mm\u001b[33m\u001b[33m\n",
      "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]33m   \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[0m\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB][33m\u001b[33m\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1404 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1204 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\u001b[0m\u001b[33m\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB][33m\u001b[33m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1430 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1474 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1279 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [78.3 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
      "Fetched 29.0 MB in 5s (5466 kB/s)33m                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "101 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  sudo\n",
      "0 upgraded, 1 newly installed, 0 to remove and 101 not upgraded.\n",
      "Need to get 821 kB of archives.\n",
      "After this operation, 2568 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sudo amd64 1.9.9-1ubuntu2.4 [821 kB]\n",
      "Fetched 821 kB in 2s (423 kB/s)[0m[33m\u001b[33m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package sudo.\n",
      "(Reading database ... 19271 files and directories currently installed.)\n",
      "Preparing to unpack .../sudo_1.9.9-1ubuntu2.4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking sudo (1.9.9-1ubuntu2.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up sudo (1.9.9-1ubuntu2.4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libexpat1-dev libpython3.10 libpython3.10-dev libpython3.10-minimal\n",
      "  libpython3.10-stdlib python3.10 python3.10-minimal zlib1g-dev\n",
      "Suggested packages:\n",
      "  python3.10-venv python3.10-doc binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  libexpat1-dev libpython3.10 libpython3.10-dev python3.10-dev zlib1g-dev\n",
      "The following packages will be upgraded:\n",
      "  libpython3.10-minimal libpython3.10-stdlib python3.10 python3.10-minimal\n",
      "4 upgraded, 5 newly installed, 0 to remove and 97 not upgraded.\n",
      "Need to get 13.0 MB of archives.\n",
      "After this operation, 29.0 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10 amd64 3.10.12-1~22.04.2 [509 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-stdlib amd64 3.10.12-1~22.04.2 [1849 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-minimal amd64 3.10.12-1~22.04.2 [2258 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-minimal amd64 3.10.12-1~22.04.2 [811 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libexpat1-dev amd64 2.4.7-1ubuntu0.2 [147 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10 amd64 3.10.12-1~22.04.2 [1949 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 zlib1g-dev amd64 1:1.2.11.dfsg-2ubuntu9.2 [164 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpython3.10-dev amd64 3.10.12-1~22.04.2 [4764 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.2 [507 kB]\n",
      "Fetched 13.0 MB in 3s (4489 kB/s)          \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "(Reading database ... 19339 files and directories currently installed.)\n",
      "Preparing to unpack .../0-python3.10_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking python3.10 (3.10.12-1~22.04.2) over (3.10.6-1~22.04.2ubuntu1) ...\n",
      "Preparing to unpack .../1-libpython3.10-stdlib_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking libpython3.10-stdlib:amd64 (3.10.12-1~22.04.2) over (3.10.6-1~22.04.2ubuntu1) ...\n",
      "Preparing to unpack .../2-python3.10-minimal_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking python3.10-minimal (3.10.12-1~22.04.2) over (3.10.6-1~22.04.2ubuntu1) ...\n",
      "Preparing to unpack .../3-libpython3.10-minimal_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking libpython3.10-minimal:amd64 (3.10.12-1~22.04.2) over (3.10.6-1~22.04.2ubuntu1) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../4-libexpat1-dev_2.4.7-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.4.7-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libpython3.10:amd64.\n",
      "Preparing to unpack .../5-libpython3.10_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking libpython3.10:amd64 (3.10.12-1~22.04.2) ...\n",
      "Selecting previously unselected package zlib1g-dev:amd64.\n",
      "Preparing to unpack .../6-zlib1g-dev_1%3a1.2.11.dfsg-2ubuntu9.2_amd64.deb ...\n",
      "Unpacking zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu9.2) ...\n",
      "Selecting previously unselected package libpython3.10-dev:amd64.\n",
      "Preparing to unpack .../7-libpython3.10-dev_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking libpython3.10-dev:amd64 (3.10.12-1~22.04.2) ...\n",
      "Selecting previously unselected package python3.10-dev.\n",
      "Preparing to unpack .../8-python3.10-dev_3.10.12-1~22.04.2_amd64.deb ...\n",
      "Unpacking python3.10-dev (3.10.12-1~22.04.2) ...\n",
      "Setting up libexpat1-dev:amd64 (2.4.7-1ubuntu0.2) ...\n",
      "Setting up libpython3.10-minimal:amd64 (3.10.12-1~22.04.2) ...\n",
      "Setting up zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu9.2) ...\n",
      "Setting up python3.10-minimal (3.10.12-1~22.04.2) ...\n",
      "Setting up libpython3.10-stdlib:amd64 (3.10.12-1~22.04.2) ...\n",
      "Setting up libpython3.10:amd64 (3.10.12-1~22.04.2) ...\n",
      "Setting up python3.10 (3.10.12-1~22.04.2) ...\n",
      "Setting up libpython3.10-dev:amd64 (3.10.12-1~22.04.2) ...\n",
      "Setting up python3.10-dev (3.10.12-1~22.04.2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /.netrc\n"
     ]
    }
   ],
   "source": [
    "# /home2/ENROOT_IMAGES/ubuntu22.04-py3.10-cuda11.8-pytorch2.1.0.dev20230513+cu118-jupyter.sqsh \n",
    "!pip install --upgrade pip\n",
    "!pip install transformers peft tensorflow accelerate bitsandbytes wandb datasets scipy scikit-learn openpyxl\n",
    "!pip install --upgrade pip\n",
    "!apt update\n",
    "!apt install sudo\n",
    "!sudo apt-get install python3.10-dev -y\n",
    "!wandb login --relogin \"96e213bec6e0b2c89e5254f9b8ab09d6270c24b3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-21 07:41:46.320882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 07:41:46.324000: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 07:41:46.365792: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 07:41:46.365817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 07:41:46.367033: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-21 07:41:46.374414: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 07:41:46.375206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 07:41:47.304966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/workspace/prompt_tuning_multi-df0202e6-a02f-43df-8161-b01421f38600.ipynb')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os, argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import huggingface_hub\n",
    "import transformers\n",
    "import wandb\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import ast\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from datasets import load_dataset, Features, Value\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor\n",
    "from peft import PromptEmbedding, PromptTuningConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(arg):\n",
    "    train_file, eval_file = 'solvook_handout_tr_10_31.csv', 'solvook_handout_val_10_31.csv'\n",
    "    train_file_path, eval_file_path = os.path.join(arg.data_dir, train_file), os.path.join(arg.data_dir, eval_file)\n",
    "    if os.path.exists(train_file_path) and os.path.exists(eval_file_path):\n",
    "        FileNotFoundError(f'There are no train or test files in {train_file_path} or {eval_file_path}')\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": train_file_path, \"eval\": eval_file_path},)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(arg):\n",
    "    test_file = 'solvook_handout_te_10_31.csv'\n",
    "    test_file_path = os.path.join(arg.data_dir, test_file)\n",
    "    if os.path.exists(test_file_path):\n",
    "        FileNotFoundError(f'There are no train or test files in {test_file_path}')\n",
    "    dataset = load_dataset('csv', data_files = {'test': test_file_path})\n",
    "    return dataset\n",
    "\n",
    "#  input_columns = ['질문', '본문', '조건', '선지', '정답']\n",
    "def preprocess_function(examples, tokenizer, is_testset = False, input_columns = ['질문'], label_columns = ['s&m'], pretrained_model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "    batch_size = len(examples['본문'])\n",
    "    # input column concatenate\n",
    "    prefix_text = '[INST] <<SYS>>\\n영어문제가 주어진다. 지시사항에 답하여라. \\n<<SYS>>\\n\\n 다음의 질문에 해당되는 \\'skill\\'과 \\'method\\'를 답하여라. '\n",
    "    input_list = [prefix_text for _ in range(batch_size)]\n",
    "    for col_name in input_columns:\n",
    "        for idx, x in enumerate(examples[col_name]):\n",
    "            if x == None:\n",
    "                continue    # 조건, 선지는 없는경우 존재\n",
    "            input_list[idx] += f'{col_name} : {x}, ' \n",
    "    # skill_candidate = '다음 선택지에서 skill을 한가지 고르면 된다. 선택지:[1.vocabulary, 2.grammar, 3.expression, 4.content, 5.context]'\n",
    "    # method_candidate = '다음 선택지에서 method를 한가지 고르면 된다. 선택지: [1. 내용 해석하기 (영-한 변환), 2. 어휘 쓰기 및 찾기, 3. 문장 쓰기, 4. 밑줄 친 부분 고쳐쓰기, 5. 선택지 내 요소들이 모두 맞는 것 찾기, 6. 내용과의 일치 여부 판단하기, 7. 요지 찾기, 8. 유추하기, 9. 주제문 찾기, 10. 순서 배열하기, 11. (글의 흐름에 맞게) 문장 배치하기, 12. 연결어 찾기, 13. 정오 여부 판단하기, 14. 잘못된 것 고치기, 15. 유사 여부 판단하기, 16. 관련 없는 문장 찾기]'\n",
    "    # quiztype_candidate = '다음 선택지에서 문제유형을 고르면 된다. 선택지:[4. 어휘 쓰기, 5. 어휘 뜻 맞히기, 10. 글의 요지(요악문 포함), 11. 글의 순서, 12. 내용 일치, 13. 내용 불일치, 15. 내용 유추, 16. 다른 용법 찾기, 17. 한/영 해석(조건 제시), 18. 무관한 문장 찾기, 19. 단어 재배열, 20. 밑줄 친 부분 고쳐쓰기, 22. 서술형(영어), 24. 서술형(조건 영어작문), 25. 서술형(한글), 26. 선택지 2개 이상 중 맞는 것 고르기(어법), 27. 선택지 2개 이상 중 맞는 것 고르기 (어휘), 28. 어법 상 맞는 것 찾기, 29. 어법 상 틀린 것 찾기, 30. 연결어 찾기, 34. 적절한 어휘 찾기, 35. 적절한 제목/주제 찾기, 36. 주어진 문장 넣기, 38. 틀린 어휘 찾기, 40. 서술형(조건 한글작문), 41. 내용 일치 (영어 질문), 42. 같은 용법 찾기, 44. 다른 어휘 찾기, 45. 같은 어휘 찾기]\\n'\n",
    "    tail = '정답 = [/INST]'\n",
    "    instruction_text = tail\n",
    "    for idx in range(batch_size):\n",
    "        input_list[idx] += instruction_text\n",
    "        \n",
    "    all_sheet = pd.read_excel('./100_Solvook_handout_DB_english (2).xlsx', sheet_name= None)\n",
    "    handout_type = all_sheet['5. handout_type(81 회의 이후 변경)']\n",
    "    handout_type.columns = handout_type.iloc[0]\n",
    "    handout_type = handout_type[1:]\n",
    "    handout_type.reset_index(drop= True, inplace=True)\n",
    "    skill_dict, method_dict = {}, {}\n",
    "    for idx in range(len(handout_type)):\n",
    "        row = handout_type.iloc[idx]\n",
    "        skill_dict[row['skill #']] = row['skill (2depth)']\n",
    "        method_dict[row['method #']] = row['method (2depth) 영어']\n",
    "\n",
    "    target_list = ['' for _ in range(batch_size)]\n",
    "    for idx in range(batch_size):\n",
    "        sm = ast.literal_eval(examples['s&m'][idx])\n",
    "        skill_index = sm[::2]\n",
    "        method_index = sm[1::2]\n",
    "        target_list[idx] += 'skill-method는 '+ '또는 '.join(['skill: ' + skill_dict[si] + ', method: '+ method_dict[mi] for si, mi in zip(skill_index, method_index)]) +'이다. </s>'\n",
    "        \n",
    "    model_inputs = tokenizer(input_list)\n",
    "    if is_testset:\n",
    "        skills = []\n",
    "        methods = []\n",
    "        for idx in range(batch_size):\n",
    "            sm = ast.literal_eval(examples['s&m'][idx])\n",
    "            skills.append([skill_dict[si] for si in sm[::2]])\n",
    "            methods.append([method_dict[mi] for mi in sm[1::2]])\n",
    "        model_inputs[\"skill\"] = skills\n",
    "        model_inputs[\"method\"] = methods\n",
    "        return model_inputs\n",
    "    \n",
    "    labels = tokenizer(target_list)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids # input_ids ... labels\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids # -100 ... labels\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i]) # input_ids + labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def collate_fn(samples):\n",
    "    batch_size = len(samples)\n",
    "    max_length = max([len(samples[i]['input_ids']) for i in range(batch_size)])\n",
    "    sample_input_ids = []\n",
    "    label_input_ids = []\n",
    "    attention_mask_ids = []\n",
    "    for sample in samples:\n",
    "        sample_input_ids.append(sample[\"input_ids\"])\n",
    "        label_input_ids.append(sample[\"labels\"])\n",
    "        attention_mask_ids.append(sample[\"attention_mask\"])\n",
    "    samples = {'input_ids': sample_input_ids, 'attention_mask': attention_mask_ids, 'labels': label_input_ids}\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = samples[\"input_ids\"][i]\n",
    "        label_input_ids = samples[\"labels\"][i]\n",
    "        attention_mask_ids = samples[\"attention_mask\"][i]\n",
    "        # padding to left in batch_size\n",
    "        samples[\"input_ids\"][i] = [0] * (max_length - len(sample_input_ids)) + sample_input_ids\n",
    "        samples[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + attention_mask_ids\n",
    "        samples[\"labels\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        # list -> tensor\n",
    "    samples[\"input_ids\"] = torch.tensor(samples[\"input_ids\"]).contiguous()\n",
    "    samples[\"attention_mask\"] = torch.tensor(samples[\"attention_mask\"]).contiguous()\n",
    "    samples[\"labels\"] = torch.tensor(samples[\"labels\"]).contiguous()\n",
    "    return samples\n",
    "\n",
    "def find_similar_index(query, value):    \n",
    "    cosine_sims = []\n",
    "    for row_A in query:\n",
    "        row_A = row_A.view(1, -1)  \n",
    "        cos_sim_row = F.cosine_similarity(row_A, value, dim=1)\n",
    "        cosine_sims.append(cos_sim_row)\n",
    "    return torch.argmax(torch.stack(cosine_sims), dim=1)\n",
    "\n",
    "def draw_tsne(matrix):\n",
    "    # Create a t-SNE model and transform the data\n",
    "    tsne = TSNE(n_components=2, perplexity = 1, random_state=42, init='random', learning_rate=200)\n",
    "    vis_dims = tsne.fit_transform(matrix)\n",
    "    x = [x for x,y in vis_dims]\n",
    "    y = [y for x,y in vis_dims]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(x, y, c=np.arange(len(matrix)), alpha=0.3)\n",
    "    for idx in range(len(matrix)):\n",
    "        plt.annotate(f'{idx+1}: prompt', (x[idx], y[idx]))\n",
    "    plt.title(\"prompt embedding t-SNE\")\n",
    "    return plt\n",
    "\n",
    "def is_rank_0() -> bool:\n",
    "    return int(os.environ.get(\"RANK\", \"0\")) == 0\n",
    "\n",
    "def pred_parse(string , pats = ['vocabulary', 'grammar', 'expression', 'content', 'context']):\n",
    "    txt_preds, num_preds = [], []\n",
    "    for pat in pats:\n",
    "        for match in re.finditer(pat, string):\n",
    "            txt_preds.append(match.group())\n",
    "    txt_preds = np.unique(txt_preds) # 중복 처리\n",
    "    if 3>= len(txt_preds)>= 1:\n",
    "        for pred in txt_preds:       \n",
    "            num_preds.append(pats.index(pred))\n",
    "    else:                           # 예외 skill label: 5\n",
    "        num_preds = len(pats)\n",
    "    return txt_preds, num_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(token='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', seed=100, device='cuda', data_dir='./', pretrained_model_name='bigscience/bloomz-560m', pretrained_model_dir=False, virtual_tokens=100, batch_size=4, epochs=50, lr=0.001, ver='ptuning2', local_rank=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgracely9901\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/eunsik12/lab/wandb/run-20231121_074615-3cq53kv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gracely9901/prompt_tuning_excer/runs/3cq53kv8' target=\"_blank\">eager-microwave-110</a></strong> to <a href='https://wandb.ai/gracely9901/prompt_tuning_excer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gracely9901/prompt_tuning_excer' target=\"_blank\">https://wandb.ai/gracely9901/prompt_tuning_excer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gracely9901/prompt_tuning_excer/runs/3cq53kv8' target=\"_blank\">https://wandb.ai/gracely9901/prompt_tuning_excer/runs/3cq53kv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--token\", type=str, default=False)\n",
    "parser.add_argument('--seed', default=100, type=int, help='seed for initializing training. ')\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "                    \n",
    "parser.add_argument(\"--data_dir\", type=str, default='./')\n",
    "parser.add_argument('--pretrained_model_name', type=str, default=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "parser.add_argument('--pretrained_model_dir', type=str, default=False, help='pretrained model ckpt')\n",
    "parser.add_argument('--virtual_tokens', type=int, default=100, help='prompt length')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=4, type=int)\n",
    "parser.add_argument('--epochs', default=50, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--lr', default=0.001, type=float, metavar='N',\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--ver', type=str, default='ptuning2')\n",
    "parser.add_argument('--local_rank', type=int)\n",
    "\n",
    "arg = parser.parse_args(args=[])\n",
    "\n",
    "arg.local_rank = 0\n",
    "arg.token = \"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\"\n",
    "arg.pretrained_model_name = \"bigscience/bloomz-560m\"\n",
    "os.chdir('/home2/eunsik12/lab')\n",
    "print(arg)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "wandb.init(project= 'prompt_tuning_excer')\n",
    "wandb.config.update(arg)\n",
    "wandb.run.name = 'Tips_prompt_tuning/bloomz-560m/multi_label'\n",
    "if not arg.token:\n",
    "    ValueError('You need to input arg \\'--token <your_token>\\'. Get your token in (https://huggingface.co/settings/tokens)')\n",
    "torch.manual_seed(arg.seed)\n",
    "huggingface_hub.login(arg.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /.cache/huggingface/datasets/csv/default-d3a28f11cce22282/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6340.60it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 24.90it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /.cache/huggingface/datasets/csv/default-d3a28f11cce22282/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 492.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /.cache/huggingface/datasets/csv/default-92af94f976b98b33/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4040.76it/s]\n",
      "Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 56.18it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /.cache/huggingface/datasets/csv/default-92af94f976b98b33/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 525.08it/s]\n",
      "Downloading tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 222/222 [00:00<00:00, 733kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.5M/14.5M [00:00<00:00, 44.5MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85.0/85.0 [00:00<00:00, 324kB/s]\n",
      "Downloading config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 715/715 [00:00<00:00, 2.42MB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.12G/1.12G [00:19<00:00, 57.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(arg)\n",
    "raw_test_dataset = get_test_dataset(arg)\n",
    "\n",
    "pretrained_model_name_or_path = arg.pretrained_model_name # \"meta-llama/Llama-2-7b-chat-hf\" https://huggingface.co/blog/llama2\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "    \n",
    "processed_datasets = dataset.map(partial(preprocess_function, tokenizer = tokenizer),\n",
    "                        batched=True,\n",
    "                        num_proc=1,\n",
    "                        remove_columns=dataset[\"train\"].column_names,\n",
    "                        keep_in_memory = True,\n",
    "                        desc=\"Tokenizing on dataset\",\n",
    "                        )\n",
    "test_dataset = raw_test_dataset.map(partial(preprocess_function, tokenizer = tokenizer, is_testset = True),\n",
    "                        batched=True,\n",
    "                        num_proc=1,\n",
    "                        remove_columns=dataset[\"train\"].column_names,\n",
    "                        keep_in_memory = True,\n",
    "                        desc=\"Tokenizing on Test dataset\",\n",
    "                        )\n",
    "\n",
    "train_dataloader = DataLoader(processed_datasets['train'], shuffle=True, collate_fn=collate_fn, batch_size=arg.batch_size, pin_memory=True)\n",
    "eval_dataloader = DataLoader(processed_datasets['eval'], shuffle=False, collate_fn=collate_fn, batch_size=arg.batch_size, pin_memory=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                        pretrained_model_name_or_path = pretrained_model_name_or_path,\n",
    "                        return_dict=True,\n",
    "                        low_cpu_mem_usage=True,)\n",
    "\n",
    "config = PromptTuningConfig(task_type=\"CAUSAL_LM\", \n",
    "                            prompt_tuning_init=\"TEXT\",\n",
    "                            num_virtual_tokens=arg.virtual_tokens,\n",
    "                            prompt_tuning_init_text=\"[1.vocabulary, 2.grammar, 3.expression, 4.content, 5.context], [1. 내용 해석하기 (영-한 변환), 2. 어휘 쓰기 및 찾기, 3. 문장 쓰기, 4. 밑줄 친 부분 고쳐쓰기, 5. 선택지 내 요소들이 모두 맞는 것 찾기, 6. 내용과의 일치 여부 판단하기, 7. 요지 찾기, 8. 유추하기, 9. 주제문 찾기, 10. 순서 배열하기, 11. (글의 흐름에 맞게) 문장 배치하기, 12. 연결어 찾기, 13. 정오 여부 판단하기, 14. 잘못된 것 고치기, 15. 유사 여부 판단하기, 16. 관련 없는 문장 찾기]\", # prompt 를 초기화 할때 넣는 임의의 text\n",
    "                            tokenizer_name_or_path=pretrained_model_name_or_path,\n",
    "                            )\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sheet = pd.read_excel('./100_Solvook_handout_DB_english (2).xlsx', sheet_name= None)\n",
    "handout_type = all_sheet['5. handout_type(81 회의 이후 변경)']\n",
    "handout_type.columns = handout_type.iloc[0]\n",
    "handout_type = handout_type[1:]\n",
    "handout_type.reset_index(drop= True, inplace=True)\n",
    "skill_dict, method_dict = {}, {}\n",
    "for idx in range(len(handout_type)):\n",
    "    row = handout_type.iloc[idx]\n",
    "    skill_dict[row['skill #']] = row['skill (2depth)']\n",
    "    method_dict[row['method #']] = row['method (2depth) 영어']\n",
    "\n",
    "skill_list = list(skill_dict[key] for key in sorted(skill_dict.keys()))\n",
    "method_list = list(method_dict[key] for key in sorted(method_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] <<SYS>>\\n영어문제가 주어진다. 지시사항에 답하여라. \\n<<SYS>>\\n\\n 다음의 질문에 해당되는 'skill'과 'method'를 답하여라. 질문 : 빈칸에 들어갈 적절한 단어를 쓰시오., 정답 = [/INST]skill-method는 skill: content, method: find the correct / find the incorrect이다. </s>\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['grammar', 'content'],\n",
       " ['find the correct / find the incorrect', 'correct the underlined'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_dataset['test']['input_ids'][945])\n",
    "test_dataset['test']['skill'][945], test_dataset['test']['method'][945]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:38<00:00,  3.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.41it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.00      0.00      0.00        60\n",
      "     grammar       0.61      0.74      0.67       286\n",
      "  expression       0.47      0.63      0.54       262\n",
      "     content       0.74      0.76      0.75      1078\n",
      "     context       0.00      0.00      0.00       143\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      1829\n",
      "   macro avg       0.36      0.43      0.39      1829\n",
      "weighted avg       0.60      0.66      0.62      1829\n",
      " samples avg       0.66      0.66      0.66      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.56      0.89      0.68       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.21      0.21      0.21       114\n",
      "               correct the underlined       0.00      0.00      0.00       139\n",
      "                   find the all-true        0.67      0.76      0.71       197\n",
      "   find the matching / the mismatched       0.00      0.00      0.00       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.00      0.00      0.00        33\n",
      "                 arrange the sequence       0.00      0.00      0.00       166\n",
      "                position the sentence       0.00      0.00      0.00       145\n",
      "           find the right conjunction       0.00      0.00      0.00        27\n",
      "find the correct / find the incorrect       0.47      0.88      0.61       664\n",
      "   find the same / find the different       0.00      0.00      0.00        10\n",
      "                  find the irrelevant       0.00      0.00      0.00        20\n",
      "\n",
      "                            micro avg       0.49      0.48      0.49      1829\n",
      "                            macro avg       0.13      0.18      0.15      1829\n",
      "                         weighted avg       0.30      0.48      0.37      1829\n",
      "                          samples avg       0.49      0.49      0.49      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:41<00:00,  3.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.41it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.00      0.00      0.00        60\n",
      "     grammar       0.50      0.00      0.01       286\n",
      "  expression       0.59      0.70      0.64       262\n",
      "     content       0.67      0.88      0.76      1078\n",
      "     context       0.71      0.41      0.52       143\n",
      "\n",
      "   micro avg       0.66      0.65      0.66      1829\n",
      "   macro avg       0.49      0.40      0.39      1829\n",
      "weighted avg       0.61      0.65      0.58      1829\n",
      " samples avg       0.66      0.66      0.66      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.59      0.89      0.71       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.43      0.27      0.33       114\n",
      "               correct the underlined       0.92      0.88      0.90       139\n",
      "                   find the all-true        0.00      0.00      0.00       197\n",
      "   find the matching / the mismatched       0.68      0.44      0.54       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.00      0.00      0.00        33\n",
      "                 arrange the sequence       0.34      0.57      0.43       166\n",
      "                position the sentence       0.00      0.00      0.00       145\n",
      "           find the right conjunction       0.00      0.00      0.00        27\n",
      "find the correct / find the incorrect       0.64      0.98      0.77       664\n",
      "   find the same / find the different       0.00      0.00      0.00        10\n",
      "                  find the irrelevant       0.00      0.00      0.00        20\n",
      "\n",
      "                            micro avg       0.60      0.59      0.60      1829\n",
      "                            macro avg       0.24      0.27      0.25      1829\n",
      "                         weighted avg       0.45      0.59      0.50      1829\n",
      "                          samples avg       0.60      0.60      0.60      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:46<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:11<00:00,  6.39it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.00      0.00      0.00        60\n",
      "     grammar       0.54      0.92      0.68       286\n",
      "  expression       0.73      0.93      0.82       262\n",
      "     content       0.91      0.79      0.84      1078\n",
      "     context       0.93      0.40      0.56       143\n",
      "\n",
      "   micro avg       0.78      0.77      0.78      1829\n",
      "   macro avg       0.62      0.61      0.58      1829\n",
      "weighted avg       0.80      0.77      0.77      1829\n",
      " samples avg       0.78      0.78      0.78      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.69      0.91      0.78       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.61      0.74      0.67       114\n",
      "               correct the underlined       0.93      0.74      0.82       139\n",
      "                   find the all-true        0.43      0.88      0.58       197\n",
      "   find the matching / the mismatched       0.92      0.35      0.51       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.00      0.00      0.00        33\n",
      "                 arrange the sequence       0.87      0.70      0.77       166\n",
      "                position the sentence       0.86      0.66      0.75       145\n",
      "           find the right conjunction       0.25      0.81      0.38        27\n",
      "find the correct / find the incorrect       0.84      0.73      0.78       664\n",
      "   find the same / find the different       0.00      0.00      0.00        10\n",
      "                  find the irrelevant       0.00      0.00      0.00        20\n",
      "\n",
      "                            micro avg       0.70      0.69      0.69      1829\n",
      "                            macro avg       0.43      0.43      0.40      1829\n",
      "                         weighted avg       0.73      0.69      0.68      1829\n",
      "                          samples avg       0.69      0.69      0.69      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:46<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.40it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.00      0.00      0.00        60\n",
      "     grammar       0.80      0.88      0.84       286\n",
      "  expression       0.94      0.85      0.90       262\n",
      "     content       0.89      0.95      0.92      1078\n",
      "     context       0.83      0.63      0.72       143\n",
      "\n",
      "   micro avg       0.88      0.87      0.87      1829\n",
      "   macro avg       0.69      0.66      0.67      1829\n",
      "weighted avg       0.85      0.87      0.86      1829\n",
      " samples avg       0.88      0.87      0.87      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.96      0.89      0.92       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.82      0.70      0.75       114\n",
      "               correct the underlined       0.98      0.86      0.91       139\n",
      "                   find the all-true        0.85      0.79      0.82       197\n",
      "   find the matching / the mismatched       0.81      0.70      0.75       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.31      0.85      0.46        33\n",
      "                 arrange the sequence       0.81      0.94      0.87       166\n",
      "                position the sentence       0.94      0.92      0.93       145\n",
      "           find the right conjunction       0.10      0.22      0.13        27\n",
      "find the correct / find the incorrect       0.88      0.88      0.88       664\n",
      "   find the same / find the different       0.33      0.10      0.15        10\n",
      "                  find the irrelevant       0.00      0.00      0.00        20\n",
      "\n",
      "                            micro avg       0.82      0.81      0.82      1829\n",
      "                            macro avg       0.52      0.52      0.51      1829\n",
      "                         weighted avg       0.83      0.81      0.81      1829\n",
      "                          samples avg       0.82      0.82      0.82      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.40it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.33      0.30      0.32        60\n",
      "     grammar       0.68      0.96      0.79       286\n",
      "  expression       0.95      0.87      0.91       262\n",
      "     content       0.95      0.88      0.92      1078\n",
      "     context       0.92      0.71      0.80       143\n",
      "\n",
      "   micro avg       0.87      0.86      0.86      1829\n",
      "   macro avg       0.77      0.74      0.75      1829\n",
      "weighted avg       0.88      0.86      0.87      1829\n",
      " samples avg       0.87      0.87      0.87      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.98      0.87      0.92       148\n",
      "         write or find the right word       0.22      0.45      0.30        22\n",
      "                     write a sentence       0.82      0.78      0.80       114\n",
      "               correct the underlined       0.97      0.80      0.88       139\n",
      "                   find the all-true        0.93      0.90      0.91       197\n",
      "   find the matching / the mismatched       0.84      0.73      0.78       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.78      0.94      0.85        33\n",
      "                 arrange the sequence       0.92      0.95      0.93       166\n",
      "                position the sentence       0.97      0.97      0.97       145\n",
      "           find the right conjunction       0.35      0.96      0.51        27\n",
      "find the correct / find the incorrect       0.89      0.88      0.89       664\n",
      "   find the same / find the different       0.36      0.80      0.50        10\n",
      "                  find the irrelevant       0.00      0.00      0.00        20\n",
      "\n",
      "                            micro avg       0.86      0.85      0.86      1829\n",
      "                            macro avg       0.60      0.67      0.62      1829\n",
      "                         weighted avg       0.87      0.85      0.86      1829\n",
      "                          samples avg       0.86      0.86      0.86      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:11<00:00,  6.38it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.59      0.17      0.26        60\n",
      "     grammar       0.95      0.84      0.89       286\n",
      "  expression       0.80      0.95      0.87       262\n",
      "     content       0.88      0.95      0.91      1078\n",
      "     context       0.99      0.48      0.65       143\n",
      "\n",
      "   micro avg       0.88      0.87      0.87      1829\n",
      "   macro avg       0.84      0.68      0.72      1829\n",
      "weighted avg       0.88      0.87      0.86      1829\n",
      " samples avg       0.88      0.87      0.88      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.75      0.97      0.85       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.72      0.75      0.74       114\n",
      "               correct the underlined       0.92      0.96      0.94       139\n",
      "                   find the all-true        0.97      0.90      0.93       197\n",
      "   find the matching / the mismatched       0.97      0.50      0.66       126\n",
      "                   find the main idea       0.00      0.00      0.00         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.97      0.94      0.95        33\n",
      "                 arrange the sequence       0.98      0.78      0.87       166\n",
      "                position the sentence       0.98      0.97      0.98       145\n",
      "           find the right conjunction       1.00      0.33      0.50        27\n",
      "find the correct / find the incorrect       0.86      0.93      0.89       664\n",
      "   find the same / find the different       0.00      0.00      0.00        10\n",
      "                  find the irrelevant       0.28      0.90      0.42        20\n",
      "\n",
      "                            micro avg       0.85      0.85      0.85      1829\n",
      "                            macro avg       0.63      0.60      0.58      1829\n",
      "                         weighted avg       0.86      0.85      0.84      1829\n",
      "                          samples avg       0.85      0.85      0.85      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.39it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.57      0.33      0.42        60\n",
      "     grammar       0.91      0.91      0.91       286\n",
      "  expression       0.98      0.86      0.92       262\n",
      "     content       0.95      0.94      0.94      1078\n",
      "     context       0.68      0.92      0.78       143\n",
      "\n",
      "   micro avg       0.91      0.90      0.90      1829\n",
      "   macro avg       0.82      0.79      0.79      1829\n",
      "weighted avg       0.91      0.90      0.90      1829\n",
      " samples avg       0.91      0.91      0.91      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.89      0.91      0.90       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.98      0.69      0.81       114\n",
      "               correct the underlined       0.85      0.96      0.90       139\n",
      "                   find the all-true        0.91      0.94      0.92       197\n",
      "   find the matching / the mismatched       0.63      0.87      0.73       126\n",
      "                   find the main idea       0.56      0.62      0.59         8\n",
      "                      make an analogy       0.10      0.10      0.10        10\n",
      "                 find the right title       0.88      0.88      0.88        33\n",
      "                 arrange the sequence       0.94      0.93      0.93       166\n",
      "                position the sentence       0.98      0.97      0.97       145\n",
      "           find the right conjunction       0.73      0.89      0.80        27\n",
      "find the correct / find the incorrect       0.94      0.90      0.92       664\n",
      "   find the same / find the different       0.67      0.20      0.31        10\n",
      "                  find the irrelevant       0.89      0.85      0.87        20\n",
      "\n",
      "                            micro avg       0.89      0.88      0.88      1829\n",
      "                            macro avg       0.73      0.71      0.71      1829\n",
      "                         weighted avg       0.89      0.88      0.88      1829\n",
      "                          samples avg       0.89      0.88      0.88      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:11<00:00,  6.39it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.42      0.43      0.43        60\n",
      "     grammar       0.93      0.91      0.92       286\n",
      "  expression       0.95      0.90      0.92       262\n",
      "     content       0.95      0.95      0.95      1078\n",
      "     context       0.85      0.86      0.85       143\n",
      "\n",
      "   micro avg       0.92      0.91      0.91      1829\n",
      "   macro avg       0.82      0.81      0.81      1829\n",
      "weighted avg       0.92      0.91      0.91      1829\n",
      " samples avg       0.92      0.91      0.91      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.94      0.89      0.91       148\n",
      "         write or find the right word       0.24      0.32      0.27        22\n",
      "                     write a sentence       0.83      0.80      0.81       114\n",
      "               correct the underlined       0.98      0.95      0.96       139\n",
      "                   find the all-true        0.97      0.93      0.95       197\n",
      "   find the matching / the mismatched       0.76      0.86      0.81       126\n",
      "                   find the main idea       1.00      0.38      0.55         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.97      0.88      0.92        33\n",
      "                 arrange the sequence       0.88      0.96      0.92       166\n",
      "                position the sentence       0.99      0.99      0.99       145\n",
      "           find the right conjunction       0.66      0.78      0.71        27\n",
      "find the correct / find the incorrect       0.93      0.92      0.92       664\n",
      "   find the same / find the different       0.50      0.10      0.17        10\n",
      "                  find the irrelevant       0.88      0.75      0.81        20\n",
      "\n",
      "                            micro avg       0.90      0.89      0.90      1829\n",
      "                            macro avg       0.77      0.70      0.71      1829\n",
      "                         weighted avg       0.90      0.89      0.89      1829\n",
      "                          samples avg       0.90      0.90      0.90      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       0.64      0.23      0.34        60\n",
      "     grammar       0.91      0.93      0.92       286\n",
      "  expression       0.88      0.96      0.92       262\n",
      "     content       0.95      0.95      0.95      1078\n",
      "     context       0.88      0.84      0.86       143\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1829\n",
      "   macro avg       0.85      0.78      0.80      1829\n",
      "weighted avg       0.92      0.92      0.91      1829\n",
      " samples avg       0.92      0.92      0.92      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.95      0.95      0.95       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.74      0.89      0.81       114\n",
      "               correct the underlined       0.98      0.95      0.96       139\n",
      "                   find the all-true        0.98      0.94      0.96       197\n",
      "   find the matching / the mismatched       0.88      0.75      0.81       126\n",
      "                   find the main idea       0.83      0.62      0.71         8\n",
      "                      make an analogy       0.17      0.40      0.24        10\n",
      "                 find the right title       0.97      0.94      0.95        33\n",
      "                 arrange the sequence       0.99      0.81      0.89       166\n",
      "                position the sentence       0.99      0.99      0.99       145\n",
      "           find the right conjunction       0.75      0.89      0.81        27\n",
      "find the correct / find the incorrect       0.90      0.96      0.93       664\n",
      "   find the same / find the different       1.00      0.20      0.33        10\n",
      "                  find the irrelevant       1.00      0.70      0.82        20\n",
      "\n",
      "                            micro avg       0.91      0.90      0.91      1829\n",
      "                            macro avg       0.81      0.73      0.75      1829\n",
      "                         weighted avg       0.91      0.90      0.90      1829\n",
      "                          samples avg       0.91      0.91      0.91      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3628/3628 [19:45<00:00,  3.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 454/454 [01:10<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  vocabulary       1.00      0.15      0.26        60\n",
      "     grammar       0.89      0.94      0.92       286\n",
      "  expression       0.96      0.93      0.94       262\n",
      "     content       0.95      0.97      0.96      1078\n",
      "     context       0.85      0.85      0.85       143\n",
      "\n",
      "   micro avg       0.93      0.92      0.93      1829\n",
      "   macro avg       0.93      0.77      0.79      1829\n",
      "weighted avg       0.93      0.92      0.92      1829\n",
      " samples avg       0.93      0.93      0.93      1829\n",
      "\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "               translate the sentence       0.93      0.95      0.94       148\n",
      "         write or find the right word       0.00      0.00      0.00        22\n",
      "                     write a sentence       0.91      0.83      0.87       114\n",
      "               correct the underlined       0.98      0.97      0.97       139\n",
      "                   find the all-true        0.99      0.75      0.85       197\n",
      "   find the matching / the mismatched       0.79      0.87      0.83       126\n",
      "                   find the main idea       0.75      0.38      0.50         8\n",
      "                      make an analogy       0.00      0.00      0.00        10\n",
      "                 find the right title       0.91      0.94      0.93        33\n",
      "                 arrange the sequence       0.95      0.96      0.96       166\n",
      "                position the sentence       0.99      0.99      0.99       145\n",
      "           find the right conjunction       0.73      0.89      0.80        27\n",
      "find the correct / find the incorrect       0.86      0.95      0.91       664\n",
      "   find the same / find the different       1.00      0.10      0.18        10\n",
      "                  find the irrelevant       1.00      0.70      0.82        20\n",
      "\n",
      "                            micro avg       0.90      0.90      0.90      1829\n",
      "                            macro avg       0.79      0.69      0.70      1829\n",
      "                         weighted avg       0.89      0.90      0.89      1829\n",
      "                          samples avg       0.90      0.90      0.90      1829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completely\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BloomForCausalLM' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~usr/local/lib/python3.10/dist-packages/peft/peft_model.py:296\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getattr__\u001b[39;49m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1630\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1630\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PeftModelForCausalLM' object has no attribute 'save'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home2/eunsik12/lab/prompt_tuning_multi.ipynb 셀 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsupercom1/home2/eunsik12/lab/prompt_tuning_multi.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m torch\u001b[39m.\u001b[39msave({\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m:model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch}, save_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsupercom1/home2/eunsik12/lab/prompt_tuning_multi.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSave completely\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsupercom1/home2/eunsik12/lab/prompt_tuning_multi.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(save_dir \u001b[39m=\u001b[39m save_dir)\n",
      "File \u001b[0;32m~usr/local/lib/python3.10/dist-packages/peft/peft_model.py:298\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(name)  \u001b[39m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model, name)\n",
      "File \u001b[0;32m~usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1630\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1629\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1630\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BloomForCausalLM' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "start_epoch = 1\n",
    "optimizer = torch.optim.Adam(model.parameters()) # Adafactor weight decay 1e−5, β2 decay 0.8\n",
    "model = model.to(device)\n",
    "\n",
    "smlb = MultiLabelBinarizer(classes=skill_list)\n",
    "skill_tar_bin = smlb.fit_transform(test_dataset['test']['skill'])\n",
    "mmlb = MultiLabelBinarizer(classes=method_list)\n",
    "method_tar_bin = mmlb.fit_transform(test_dataset['test']['method'])\n",
    "\n",
    "for epoch in range(start_epoch, arg.epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        \n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    example_input= tokenizer(\n",
    "        f\"[INST] <<SYS>>\\n영어문제가 주어진다. 지시사항에 답하여라. \\n<<SYS>>\\n\\n 다음의 질문에 해당되는 'skill'과 'method'를 답하여라. 질문 : 밑줄 친 ➀ ~ ➄ 중 어법상 틀린 것을 찾아 바르게 고치시오., 정답 = [/INST]\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    out = model.generate(input_ids=example_input[\"input_ids\"], attention_mask=example_input[\"attention_mask\"], max_new_tokens=30, eos_token_id=3)\n",
    "    gen_text = tokenizer.decode(out[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    label_text = \"[INST] <<SYS>>\\n영어문제가 주어진다. 지시사항에 답하여라. \\n<<SYS>>\\n\\n 다음의 질문에 해당되는 'skill'과 'method'를 답하여라. 질문 : 밑줄 친 ➀ ~ ➄ 중 어법상 틀린 것을 찾아 바르게 고치시오., 정답 = [/INST] skill-method는 skill: grammar, method: find the correct / find the incorrect 또는 skill: content, method: correct the underlined이다. </s>\"\n",
    "    text_table = wandb.Table(columns=[\"epoch\", 'gen_text', 'label_text'])\n",
    "    text_table.add_data(epoch, gen_text, label_text)\n",
    "    wandb.log({\"generation_example\": text_table})\n",
    "\n",
    "    skill_pred, method_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataset['test']:\n",
    "            input_ids = torch.tensor([batch['input_ids']]).to(device)\n",
    "            attention_mask= torch.tensor([batch['attention_mask']]).to(device)\n",
    "            input_length = len(tokenizer.decode(input_ids[0]))\n",
    "            out = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens= 20, eos_token_id=3)\n",
    "            gen_sentence = tokenizer.decode(out[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            skill_txt_preds, skill_num_pred = pred_parse(gen_sentence[input_length:], pats = skill_list)\n",
    "            method_txt_preds, method_num_pred = pred_parse(gen_sentence[input_length:], pats = method_list)\n",
    "            skill_pred.append(skill_txt_preds)\n",
    "            method_pred.append(method_txt_preds)\n",
    "            \n",
    "    skill_pred_bin = smlb.transform(skill_pred)\n",
    "    print(classification_report(skill_tar_bin, skill_pred_bin, target_names=skill_list))\n",
    "    \n",
    "    method_pred_bin = mmlb.transform(method_pred)\n",
    "    print(classification_report(method_tar_bin, method_pred_bin, target_names=method_list))\n",
    "    \n",
    "    metrics = [precision_score, recall_score, f1_score]\n",
    "    averages = ['micro', 'macro', 'weighted']\n",
    "    skill_metrics, method_metrics = [], []\n",
    "    for metric in metrics:\n",
    "        for average_type in averages:\n",
    "            skill_metrics.append(metric(skill_tar_bin, skill_pred_bin, average=average_type))\n",
    "            method_metrics.append(metric(method_tar_bin, method_pred_bin, average=average_type))\n",
    "    output_dict = {}\n",
    "    i = 0\n",
    "\n",
    "    for metric_name in ['precision_score_', 'recall_score_', 'f1_score_']:\n",
    "        for average_type in averages:\n",
    "            output_dict['skill_'+metric_name+average_type] = skill_metrics[i]\n",
    "            output_dict['method_'+metric_name+average_type] = method_metrics[i]\n",
    "\n",
    "    output_dict.update({'train_epoch_loss':train_epoch_loss, 'eval_epoch_loss': eval_epoch_loss}) \n",
    "    wandb.log(output_dict, step=epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        current_time = datetime.datetime.now()\n",
    "        time_format = \"%m-%d_%H-%M\"\n",
    "        formatted_time = current_time.strftime(time_format)\n",
    "        model_name = f\"{epoch}epoch_{formatted_time}.pt\"\n",
    "        save_dir = os.path.join(arg.data_dir, 'prompt_'+model_name)\n",
    "        torch.save({'model_state_dict':model.state_dict(), 'epoch': epoch}, save_dir)\n",
    "        print('Save completely')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
